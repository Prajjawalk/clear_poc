## 10. LLM query service

The LLM (Large Language Model) query service provides a flexible, provider-agnostic interface for integrating AI-powered natural language processing capabilities throughout the EWAS platform. Built with a plugin architecture, it enables seamless integration of various LLM providers while maintaining consistent interfaces and operational controls.

The service abstracts the complexity of working with different LLM providers, offering unified query interfaces, automatic failover mechanisms, intelligent caching, and comprehensive monitoring. This design ensures that EWAS components can leverage LLM capabilities for tasks such as alert content generation, data analysis, and natural language understanding without being tightly coupled to specific AI providers.

### 10.1 Key concepts

#### Provider abstraction layer
The **provider abstraction layer** implements a plugin-based architecture where different LLM providers (LiteLLM, OpenAI, Anthropic, local models) can be integrated through a common interface. This abstraction ensures that the platform can adapt to changing AI landscapes without requiring modifications to consuming applications.

Each provider plugin implements a standardized interface including methods for synchronous queries, streaming responses, token estimation, and configuration validation. The abstraction layer handles provider-specific authentication, request formatting, and response parsing, presenting a unified API to consuming applications.

#### LiteLLM integration
**LiteLLM** serves as the primary provider implementation, offering a unified interface to multiple LLM models through its proxy service. LiteLLM simplifies model management by providing consistent APIs across different providers (OpenAI, Anthropic, Cohere, Hugging Face) while handling authentication, rate limiting, and load balancing at the proxy level.

The integration supports:
- **Model flexibility**: Switch between models without code changes
- **Cost optimization**: Route queries to appropriate models based on complexity
- **Fallback chains**: Automatic failover to alternative models on failure
- **Local deployment**: Support for self-hosted models through LiteLLM proxy
- **Token management**: Unified token counting across different model tokenizers

#### Query processing pipeline
The **query processing pipeline** manages the complete lifecycle of LLM queries from request to response. This pipeline includes input validation, prompt engineering, provider selection, query execution, response processing, and result caching. Each stage is designed to be extensible and configurable.

The pipeline implements several optimization strategies:
- **Prompt templates**: Reusable templates with variable substitution
- **Context management**: Automatic context window management for long conversations
- **Response validation**: Schema-based validation of structured outputs
- **Error recovery**: Automatic retry with exponential backoff for transient failures
- **Cost tracking**: Token usage monitoring and cost estimation

#### Caching strategy
The **caching system** reduces latency and costs by storing responses for identical or semantically similar queries. The system implements multiple caching levels:

- **Exact match caching**: Hash-based caching for identical prompts
- **Semantic caching**: (Future) Embedding-based similarity matching
- **Partial caching**: Cache reusable prompt components
- **TTL management**: Configurable expiration based on query type
- **Cache warming**: Precompute common queries during low-usage periods

Cache keys are generated using SHA-256 hashes of normalized prompts, ensuring privacy while enabling efficient lookups. The system tracks cache hit rates and automatically adjusts TTL values based on usage patterns.

#### Rate limiting and quotas
**Rate limiting** prevents service abuse and manages costs through configurable limits at multiple levels:

- **Global limits**: System-wide request caps per time window
- **User-based limits**: Per-user or per-application quotas
- **Model-specific limits**: Different limits for different model tiers
- **Token-based limits**: Limits based on token consumption rather than request count
- **Priority queuing**: High-priority requests bypass standard rate limits

The rate limiting system uses Redis for distributed counting, ensuring accurate limits across multiple service instances. When limits are reached, requests are queued or rejected based on priority and configuration.

### 10.2 Architecture

The LLM query service follows a layered architecture with clear separation between interface, business logic, and provider implementations.

<figure>
<figcaption>LLM query service architecture</figcaption>
<img src="diagrams/llm_service_architecture.drawio.png" />
</figure>

#### Service layers
1. **API Layer**: REST endpoints and Django integration points
2. **Service Layer**: Core business logic, caching, and rate limiting
3. **Provider Layer**: Plugin-based provider implementations
4. **Infrastructure Layer**: Redis cache, database persistence, monitoring

#### Component interaction flow
1. **Request Reception**: API endpoint receives query with parameters
2. **Cache Check**: System checks for cached response
3. **Rate Limit Validation**: Verify request is within quota limits
4. **Provider Selection**: Choose appropriate provider based on availability and cost
5. **Query Execution**: Send formatted request to selected provider
6. **Response Processing**: Parse and validate provider response
7. **Cache Storage**: Store response for future use if cacheable
8. **Result Return**: Return processed response to caller

#### Plugin architecture
The plugin system enables dynamic provider registration and discovery:

```
llm_service/
├── providers/
│   ├── base.py           # Abstract base provider class
│   ├── litellm.py        # LiteLLM provider implementation
│   ├── openai.py         # Direct OpenAI provider (future)
│   └── anthropic.py      # Direct Anthropic provider (future)
├── registry.py           # Provider registration and discovery
└── service.py            # Main service orchestration
```

Each provider plugin must implement:
- `query()`: Synchronous query execution
- `stream_query()`: Streaming response support
- `validate_config()`: Configuration validation
- `get_info()`: Provider metadata and capabilities
- `estimate_tokens()`: Token count estimation

### 10.3 Data model

<figure>
<figcaption>LLM service data model</figcaption>
<img src="diagrams/llm_service_model.drawio.png" />
</figure>

#### QueryLog
The **QueryLog** model tracks all LLM queries for audit, analysis, and debugging purposes.

**Fields:**
- `id`: Primary key (BigAutoField)
- `provider`: Provider identifier (CharField, max_length=50)
- `model`: Model name used (CharField, max_length=100)
- `prompt_hash`: SHA-256 hash of prompt for privacy (CharField, max_length=64)
- `tokens_input`: Input token count (IntegerField)
- `tokens_output`: Output token count (IntegerField)
- `total_tokens`: Total tokens used (IntegerField)
- `response_time_ms`: Response time in milliseconds (IntegerField)
- `success`: Query success status (BooleanField)
- `error_message`: Error details if failed (TextField, null=True)
- `cost_estimate`: Estimated cost in USD (DecimalField, null=True)
- `user`: User who made the query (ForeignKey, null=True)
- `application`: Calling application identifier (CharField, max_length=100)
- `created_at`: Query timestamp (DateTimeField, auto_now_add=True)
- `metadata`: Additional query metadata (JSONField)

**Indexes:**
- Primary index on id
- Index on (provider, created_at) for provider analytics
- Index on (application, created_at) for usage tracking
- Index on prompt_hash for duplicate detection

#### CachedResponse
The **CachedResponse** model stores cached LLM responses for performance optimization.

**Fields:**
- `id`: Primary key (BigAutoField)
- `cache_key`: Unique cache identifier (CharField, max_length=64, unique=True)
- `provider`: Provider that generated response (CharField, max_length=50)
- `model`: Model used (CharField, max_length=100)
- `response_text`: Cached response content (TextField)
- `response_metadata`: Response metadata (JSONField)
- `created_at`: Cache entry creation (DateTimeField, auto_now_add=True)
- `expires_at`: Cache expiration time (DateTimeField)
- `hit_count`: Number of cache hits (IntegerField, default=0)
- `last_accessed`: Last cache hit timestamp (DateTimeField, auto_now=True)

**Indexes:**
- Unique index on cache_key for fast lookups
- Index on expires_at for cleanup operations
- Index on (provider, model, created_at) for cache analytics

#### ProviderConfig
The **ProviderConfig** model stores provider-specific configuration.

**Fields:**
- `id`: Primary key (AutoField)
- `provider_name`: Unique provider identifier (CharField, max_length=50, unique=True)
- `is_active`: Provider availability status (BooleanField, default=True)
- `priority`: Provider selection priority (IntegerField, default=100)
- `config`: Provider-specific configuration (JSONField)
- `rate_limit`: Requests per minute limit (IntegerField, null=True)
- `token_limit`: Tokens per day limit (IntegerField, null=True)
- `created_at`: Configuration creation (DateTimeField, auto_now_add=True)
- `updated_at`: Last configuration update (DateTimeField, auto_now=True)

### 10.4 Implementation

#### Application structure
The LLM service is implemented as a standalone Django application with pluggable provider support.

```
llm_service/
├── __init__.py
├── apps.py                    # Django app configuration
├── models.py                  # Data models
├── admin.py                   # Admin interface
├── exceptions.py              # Custom exceptions
├── providers/
│   ├── __init__.py
│   ├── base.py               # Abstract base provider
│   └── litellm.py            # LiteLLM implementation
├── registry.py               # Provider registry
├── service.py                # Main service class
├── cache.py                  # Caching implementation
├── rate_limiter.py           # Rate limiting logic
├── validators.py             # Input/output validation
├── utils.py                  # Helper utilities
├── management/
│   └── commands/
│       ├── test_llm.py      # Test connectivity
│       └── clear_llm_cache.py # Cache management
├── templates/
│   └── llm_service/
│       └── test_interface.html # Debug interface
└── tests/
    ├── test_providers.py
    ├── test_service.py
    └── test_cache.py
```

#### Configuration
Service configuration through Django settings:

```python
LLM_SERVICE = {
    'DEFAULT_PROVIDER': 'litellm',
    'PROVIDERS': {
        'litellm': {
            'CLASS': 'llm_service.providers.litellm.LiteLLMProvider',
            'API_KEY_ENV': 'LITELLM_API_KEY',
            'BASE_URL': 'http://localhost:4000',
            'MODEL': 'gpt-4',
            'TIMEOUT': 30,
            'MAX_RETRIES': 3,
        }
    },
    'CACHE': {
        'ENABLED': True,
        'TTL_SECONDS': 3600,
        'MAX_SIZE_MB': 100,
    },
    'RATE_LIMITS': {
        'GLOBAL_RPM': 100,
        'USER_RPM': 20,
        'TOKEN_DAILY_LIMIT': 1000000,
    }
}
```

#### Usage patterns
Basic service usage:

```python
from llm_service import LLMService

# Simple query
service = LLMService()
response = service.query("Analyze this humanitarian data: ...")

# With specific configuration
response = service.query(
    prompt="Translate to Arabic: Emergency evacuation required",
    provider="litellm",
    model="gpt-3.5-turbo",
    temperature=0.3,
    max_tokens=200,
    cache=True
)

# Streaming response
for chunk in service.stream_query("Generate detailed report"):
    process_chunk(chunk)
```

### 10.5 API endpoints

The LLM service exposes REST APIs for external integration and monitoring.

#### Query endpoint
**POST** `/llm/api/query/`

Execute an LLM query with specified parameters.

**Request body:**
```json
{
    "prompt": "string",
    "provider": "string (optional)",
    "model": "string (optional)",
    "temperature": "float (optional, 0-1)",
    "max_tokens": "integer (optional)",
    "cache": "boolean (optional, default: true)",
    "stream": "boolean (optional, default: false)",
    "metadata": "object (optional)"
}
```

**Response:**
```json
{
    "response": "string",
    "provider": "string",
    "model": "string",
    "tokens_used": "integer",
    "cache_hit": "boolean",
    "response_time_ms": "integer"
}
```

#### Provider status endpoint
**GET** `/llm/api/providers/`

List available providers and their status.

**Response:**
```json
{
    "providers": [
        {
            "name": "litellm",
            "active": true,
            "models": ["gpt-4", "gpt-3.5-turbo"],
            "rate_limit": 100,
            "requests_today": 45,
            "avg_response_time_ms": 1200
        }
    ]
}
```

#### Usage statistics endpoint
**GET** `/llm/api/stats/`

Retrieve usage statistics and performance metrics.

**Query parameters:**
- `period`: Time period (day, week, month)
- `provider`: Filter by provider
- `application`: Filter by application

**Response:**
```json
{
    "period": "day",
    "total_queries": 156,
    "successful_queries": 154,
    "cache_hits": 45,
    "total_tokens": 45000,
    "estimated_cost": 2.45,
    "avg_response_time_ms": 980,
    "top_applications": [
        {"name": "alert_framework", "queries": 89},
        {"name": "data_pipeline", "queries": 67}
    ]
}
```

### 10.6 Web interface

The service includes a web interface for testing and monitoring LLM interactions.

#### Test interface
An interactive testing interface allows administrators to:
- Execute test queries against different providers
- Compare responses from multiple models
- Test prompt templates with variable substitution
- Validate response formats
- Monitor real-time performance metrics

#### Monitoring dashboard
The monitoring dashboard provides:
- Real-time query volume and response time graphs
- Provider availability status
- Cache hit rate visualization
- Token usage tracking
- Cost estimation and budget monitoring
- Error rate trends and alerts

#### Administration interface
Django admin integration for:
- Provider configuration management
- Cache inspection and management
- Query log review and analysis
- Rate limit adjustment
- User quota management
